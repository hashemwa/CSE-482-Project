{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ym646kVe-D-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4014b49-9bdd-4d4a-e1f8-d75e0dc7449e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from collections import Counter\n",
        "from nltk.util import bigrams\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('twitter_training.csv', names=['tweet_id', 'subject', 'sentiment', 'review_text'])"
      ],
      "metadata": {
        "id": "uc2hH_2aIHBY"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cleaning\n",
        "\n",
        "ENGLISH_STOPWORDS_SET = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords_from_list(token_list):\n",
        "    \"\"\"\n",
        "    Filters a list of tokens, returning only those not in the stopword set.\n",
        "    \"\"\"\n",
        "    # This list comprehension is the efficient way to filter tokens\n",
        "    return [word for word in token_list if word not in ENGLISH_STOPWORDS_SET]\n",
        "\n",
        "def remove_punctuation_from_list(token_list):\n",
        "    \"\"\"\n",
        "    Filters a list of tokens, returning only those not in the stopword set.\n",
        "    \"\"\"\n",
        "    # This list comprehension is the efficient way to filter tokens\n",
        "    return [word for word in token_list if word not in punctuation]\n",
        "\n",
        "df = df[df[\"sentiment\"].isin([\"Positive\", \"Negative\"])]\n",
        "\n",
        "df[\"review_text\"] = df[\"review_text\"].str.lower()\n",
        "\n",
        "df = df.dropna(subset= [\"review_text\"])\n",
        "\n",
        "# Remove whitespace\n",
        "df[\"cleaned_text\"] = df[\"review_text\"].str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "df[\"tokens\"] = df[\"cleaned_text\"].apply(word_tokenize)\n",
        "\n",
        "# remove punctuation\n",
        "df[\"tokens\"] = df[\"tokens\"].apply(remove_punctuation_from_list)\n",
        "\n",
        "df[\"tokens\"] = df[\"tokens\"].apply(remove_stopwords_from_list)\n",
        "\n",
        "df = df.drop_duplicates(subset = [\"review_text\", \"tokens\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "5n7ZiMtJIwW3"
      },
      "execution_count": 56,
      "outputs": []
    }
  ]
}