{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym646kVe-D-e",
        "outputId": "a4014b49-9bdd-4d4a-e1f8-d75e0dc7449e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\windo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\windo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from collections import Counter\n",
        "from nltk.util import bigrams\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uc2hH_2aIHBY"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('twitter_training.csv', names=['tweet_id', 'subject', 'sentiment', 'review_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5n7ZiMtJIwW3"
      },
      "outputs": [],
      "source": [
        "## Cleaning\n",
        "\n",
        "df = df[df[\"sentiment\"].isin([\"Positive\", \"Negative\"])]\n",
        "\n",
        "# 3. Basic String Cleaning\n",
        "df[\"review_text\"] = df[\"review_text\"].str.lower()\n",
        "df = df.dropna(subset=[\"review_text\"])\n",
        "df[\"cleaned_text\"] = df[\"review_text\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
        "\n",
        "# 4. Drop Duplicates (Do this BEFORE tokenizing to avoid crashes)\n",
        "df = df.drop_duplicates(subset=[\"cleaned_text\"])\n",
        "\n",
        "# 5. Define Cleaning Functions\n",
        "ENGLISH_STOPWORDS_SET = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_tokens(token_list):\n",
        "    # Keeps words that are Alphanumeric AND not in stopwords\n",
        "    # .isalnum() removes punctuation effectively\n",
        "    return [word for word in token_list if word.isalnum() and word not in ENGLISH_STOPWORDS_SET]\n",
        "\n",
        "# 6. Tokenize & Apply Cleaning\n",
        "df[\"tokens\"] = df[\"cleaned_text\"].apply(word_tokenize)\n",
        "df[\"tokens\"] = df[\"tokens\"].apply(clean_tokens)\n",
        "\n",
        "# 7. Create \"Final Text\" for Yernar (TF-IDF needs strings, not lists)\n",
        "df[\"final_text\"] = df[\"tokens\"].str.join(' ')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
